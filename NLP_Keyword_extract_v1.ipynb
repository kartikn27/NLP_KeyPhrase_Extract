{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kartik Spark Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/spark-2.1.0-bin-cdh5.9.1/\")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "    config('spark.executor.memory', '5g').\\\n",
    "    config('spark.driver.memory', '5g').\\\n",
    "    config('spark.driver.maxResultSize', '3g').\\\n",
    "    config('spark.dynamicAllocation.maxExecutors', 20).\\\n",
    "getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rahul Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/cloudera/parcels/SPARK2/lib/spark2/')\n",
    "\n",
    "# import the library\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# for Spark 2.0, we have a unified entry point to the cluster\n",
    "spark = SparkSession.builder.\\\n",
    "    getOrCreate()\n",
    "\n",
    "# for previous versions, we can simulate SparkContext and SQLContext\n",
    "sc = spark.sparkContext\n",
    "sqlContext = spark\n",
    "\n",
    "# Display information about current execution\n",
    "spark.conf.get('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addPyFile('rake.py')\n",
    "spark.sparkContext.addFile('SmartStoplist.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rake\n",
    "rake_object = rake.Rake(\"SmartStoplist.txt\", 3, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_words(text, good_tags=set(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])):\n",
    "    import itertools, nltk, string\n",
    "\n",
    "    # exclude candidates that are stop words or entirely punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokenize and POS-tag words\n",
    "    tagged_words = itertools.chain.from_iterable(nltk.pos_tag_sents(nltk.word_tokenize(sent)\n",
    "                                                                    for sent in nltk.sent_tokenize(text)))\n",
    "    # filter on certain POS tags and lowercase all words\n",
    "    candidates = [word.lower() for word, tag in tagged_words\n",
    "                  if tag in good_tags and word.lower() not in stop_words\n",
    "                  and not all(char in punct for char in word)]\n",
    "\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_keyphrases_by_textrank(text, n_keywords=0.05):\n",
    "    from itertools import takewhile, tee\n",
    "    import networkx, nltk\n",
    "    \n",
    "    # tokenize for all words, and extract *candidate* words\n",
    "    words = [word.lower()\n",
    "             for sent in nltk.sent_tokenize(text)\n",
    "             for word in nltk.word_tokenize(sent)]\n",
    "    candidates = extract_candidate_words(text)\n",
    "    # build graph, each node is a unique candidate\n",
    "    graph = networkx.Graph()\n",
    "    graph.add_nodes_from(set(candidates))\n",
    "    # iterate over word-pairs, add unweighted edges into graph\n",
    "    def pairwise(iterable):\n",
    "        \"\"\"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\"\"\n",
    "        a, b = tee(iterable)\n",
    "        next(b, None)\n",
    "        return zip(a, b)\n",
    "    for w1, w2 in pairwise(candidates):\n",
    "        if w2:\n",
    "            graph.add_edge(*sorted([w1, w2]))\n",
    "    # score nodes using default pagerank algorithm, sort by score, keep top n_keywords\n",
    "    ranks = networkx.pagerank(graph)\n",
    "    if 0 < n_keywords < 1:\n",
    "        n_keywords = int(round(len(candidates) * n_keywords))\n",
    "    word_ranks = {word_rank[0]: word_rank[1]\n",
    "                  for word_rank in sorted(ranks.items(), key=lambda x: x[1], reverse=True)[:n_keywords]}\n",
    "    keywords = set(word_ranks.keys())\n",
    "    # merge keywords into keyphrases\n",
    "    keyphrases = {}\n",
    "    j = 0\n",
    "    for i, word in enumerate(words):\n",
    "        if i < j:\n",
    "            continue\n",
    "        if word in keywords:\n",
    "            kp_words = list(takewhile(lambda x: x in keywords, words[i:i+10]))\n",
    "            avg_pagerank = sum(word_ranks[w] for w in kp_words) / float(len(kp_words))\n",
    "            keyphrases[' '.join(kp_words)] = avg_pagerank\n",
    "            # counter as hackish way to ensure merged keyphrases are non-overlapping\n",
    "            j = i + len(kp_words)\n",
    "    \n",
    "    return sorted(keyphrases.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_features(candidates, doc_text, doc_excerpt, doc_title, doc_file):\n",
    "    import collections, math, nltk, re\n",
    "    \n",
    "    candidate_scores = collections.OrderedDict()\n",
    "    \n",
    "    # get word counts for document\n",
    "    doc_word_counts = collections.Counter(word.lower()\n",
    "                                          for sent in nltk.sent_tokenize(doc_text)\n",
    "                                          for word in nltk.word_tokenize(sent))\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        \n",
    "        pattern = re.compile(r'\\b'+re.escape(candidate)+r'(\\b|[,;.!?]|\\s)', re.IGNORECASE)\n",
    "        \n",
    "        # frequency-based\n",
    "        # number of times candidate appears in document\n",
    "        cand_doc_count = len(pattern.findall(doc_text))\n",
    "        # count could be 0 for multiple reasons; shit happens in a simplified example\n",
    "        if not cand_doc_count:\n",
    "            print('**WARNING:', candidate, 'not found!')\n",
    "            continue\n",
    "    \n",
    "        # statistical\n",
    "        candidate_words = candidate.split()\n",
    "        max_word_length = max(len(w) for w in candidate_words)\n",
    "        term_length = len(candidate_words)\n",
    "        # get frequencies for term and constituent words\n",
    "        sum_doc_word_counts = float(sum(doc_word_counts[w] for w in candidate_words))\n",
    "        try:\n",
    "            # lexical cohesion doesn't make sense for 1-word terms\n",
    "            if term_length == 1:\n",
    "                lexical_cohesion = 0.0\n",
    "            else:\n",
    "                lexical_cohesion = term_length * (1 + math.log(cand_doc_count, 10)) * cand_doc_count / sum_doc_word_counts\n",
    "        except (ValueError, ZeroDivisionError) as e:\n",
    "            lexical_cohesion = 0.0\n",
    "        \n",
    "        # positional\n",
    "        # found in title, key excerpt\n",
    "        in_title = 1 if pattern.search(doc_title) else 0\n",
    "        in_excerpt = 1 if pattern.search(doc_excerpt) else 0\n",
    "        # first/last position, difference between them (spread)\n",
    "        doc_text_length = float(len(doc_text))\n",
    "        first_match = pattern.search(doc_text)\n",
    "        abs_first_occurrence = first_match.start() / doc_text_length\n",
    "        if cand_doc_count == 1:\n",
    "            spread = 0.0\n",
    "            abs_last_occurrence = abs_first_occurrence\n",
    "        else:\n",
    "            for last_match in pattern.finditer(doc_text):\n",
    "                pass\n",
    "            abs_last_occurrence = last_match.start() / doc_text_length\n",
    "            spread = abs_last_occurrence - abs_first_occurrence\n",
    "\n",
    "        candidate_scores[candidate] = {'document': doc_file,\n",
    "                                       'term_count': cand_doc_count,\n",
    "                                       'term_length': term_length, \n",
    "                                       'max_word_length': max_word_length,\n",
    "                                       'spread': spread, \n",
    "                                       'lexical_cohesion': lexical_cohesion,\n",
    "                                       'in_excerpt': in_excerpt, \n",
    "                                       'in_title': in_title,\n",
    "                                       'abs_first_occurrence': abs_first_occurrence,\n",
    "                                       'abs_last_occurrence': abs_last_occurrence}\n",
    "\n",
    "    return candidate_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path_chunk = '/users/kanagre/AutomaticKeyphraseExtraction/SemEval2010/train/'\n",
    "import os\n",
    "filename_list = []\n",
    "for subdir, dirs, files in os.walk(directory_path_chunk):\n",
    "    for file in sorted(files):\n",
    "        if file.endswith('.txt.final'):            \n",
    "            filename_list.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['Evaluating Adaptive Resource Management for Distributed Real-Time Embedded Systems', 'Demonstration of Grid-Enabled Ensemble Kalman Filter Data Assimilation Methodology for Reservoir Characterization', 'MSP: Multi-Sequence Positioning of Wireless Sensor Nodes', 'StarDust: A Flexible Architecture for Passive Localization in Wireless Sensor Networks', 'TSAR: A Two Tier Sensor Storage Architecture Using Interval Skip Graphs', 'Multi-dimensional Range Queries in Sensor Networks', 'Evaluating Opportunistic Routing Protocols with Large Realistic Contact Traces', 'CenWits: A Sensor-Based Loosely Coupled Search and Rescue System Using Witnesses', 'Fairness in Dead-Reckoning based Distributed Multi-Player Games', 'Globally Synchronized Dead-Reckoning with Local Lag for Continuous Distributed Multiplayer Games', 'Remote Access to Large Spatial Databases', 'Context Awareness for Group Interaction Support', 'A Hierarchical Process Execution Support for Grid Computing', 'Congestion Games with Load-Dependent Failures: Identical Resources','A Scalable Distributed Information Management System','Authority Assignment in Distributed Multi-Player Proxy-based Games','Network Monitors and Contracting Systems: Competition and Innovation','Shooter Localization and Weapon Classification with Soldier-Wearable Networked Sensors', 'Heuristics-Based Scheduling of Composite Web Service Workloads', 'A Holistic Approach to High-Performance Computing: Xgrid Experience', 'An Evaluation of Availability Latency in Carrier-based Vehicular ad-hoc Networks','pTHINC: A Thin-Client Architecture for Mobile Wireless Web','A Point-Distribution Index and Its Application to Sensor-Grouping in Wireless Sensor Networks','GUESS: Gossiping Updates for Efficient Spectrum Sensing','Adapting Asynchronous Messaging Middleware to ad-hoc Networking','Composition of a DIDS by Integrating Heterogeneous IDSs on Grids','Assured Service Quality by Improved Fault Management Service-Oriented Event Correlation','Tracking Immediate Predecessors in Distributed Computations','An Architectural Framework and a Middleware for Cooperating Smart Components','A Cross-Layer Approach to Resource Discovery and Distribution in Mobile ad-hoc Networks','Consistency-preserving Caching of Dynamic Database Content','Adaptive Duty Cycling for Energy Harvesting Systems','Concept and Architecture of a Pervasive Document Editing and Managing System','Selfish Caching in Distributed Systems: A Game-Theoretic Analysis','AdaRank: A Boosting Algorithm for Information Retrieval','Relaxed Online SVMs for Spam Filtering','DiffusionRank: A Possible Penicillin for Web Spamming','Cross-Lingual Query Suggestion Using Query Logs of Different Languages','HITS on the Web: How does it Compare?','HITS Hits  TRECExploring IR Evaluation Results with Network Analysis','Combining Content and Link for Classification using Matrix Factorization','A Time Machine for Text Search','Query Performance Prediction in Web Search Environments','Broad Expertise Retrieval in Sparse Data Environments','A Semantic Approach to Contextual Advertising','A New Approach for Evaluating Query Expansion: Query-Document Term Mismatch','Performance Prediction Using Spatial Autocorrelation','An Outranking Approach for Rank Aggregation in Information Retrieval','Vocabulary Independent Spoken Term Detection','Context Sensitive Stemming for Web Search','Knowledge-intensive Conceptual Retrieval and Passage Extraction of Biomedical Literature', 'A Frequency-based and a Poisson-based Definition of the Probability of Being Informative', 'Impedance Coupling in Content-targeted Advertising', 'Implicit User Modeling for Personalized Search','Location based Indexing Scheme for DAYS','Machine Learning for Information Architecture in a Large Governmental Website','Ranking Web Objects from Multiple Communities','Unified Utility Maximization Framework for Resource Selection','Automatic Extraction of Titles from General Documents using Machine Learning','Beyond PageRank: Machine Learning for Static Ranking','Distance Measures for MPEG-7-based Retrieval','Downloading Textual Hidden Web Content Through Keyword Queries','Estimating the Global PageRank of Web Communities','Event Threading within News Topics','Learning User Interaction Models for Predicting Web Search Result Preferences','Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation','Controlling Overlap in Content-Oriented XML Retrieval','Context-Sensitive Information Retrieval Using Implicit Feedback','Improving Web Search Ranking by Incorporating User Behavior Information','Handling Locations in Search Engine Queries','A Study of Factors Affecting the Utility of Implicit Relevance Feedback','Feature Representation for Effective Action-Item Detection','Using Asymmetric Distributions to Improve Text Classifier Probability Estimates','A Framework for Agent-Based Distributed Machine Learning and Data Mining','Bidding Algorithms for a Distributed Combinatorial Auction', 'A Complete Distributed Constraint Optimization Method For Non-Traditional Pseudotree Arrangements','Dynamics Based Control with an Application to Area-Sweeping Problems','Implementing Commitment-Based Interactions','Modular Interpreted Systems','Operational Semantics of Multiagent Interactions','Normative System Games','A Multilateral Multi-issue Negotiation Protocol','Agents, Beliefs, and Plausible Behavior in a Temporal Setting','Learning and Joint Deliberation through Argumentation in Multi-Agent Systems','A Unified and General Framework for Argumentation-based Negotiation','A Randomized Method for the Shapley Value for the Voting Game','Approximate and Online Multi-Issue Negotiation','Searching for Joint Gains in Automated Negotiations Based on Multi-criteria Decision Making Theory','Unifying Distributed Constraint Algorithms in a BDI Negotiation Framework','Rumours and Reputation: Evaluating Multi-Dimensional Trust within a Decentralised Reputation System','An Efficient Heuristic Approach for Security Against Multiple Adversaries','An Agent-Based Approach for Privacy-Preserving Recommender Systems','On the Benefits of Cheating by Self-Interested Agents in Vehicular Networks','Distributed Agent-Based Air Traffic Flow Management','A Q-decomposition and Bounded RTDP Approach to Resource Allocation','Combinatorial Resource Scheduling for Multiagent MDPs','Organizational Self-Design in Semi-dynamic Environments','Graphical Models for Online Solutions to Interactive POMDPs','Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies','On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints','A Multi-Agent System for Building Dynamic Ontologies','A Formal Model for Situated Semantic Alignment','Learning Consumer Preferences Using Semantic Similarity','Exchanging Reputation Values among Heterogeneous Agent Reputation Models: An Experience on ART Testbed','On the relevance of utterances in formal inter-agent dialogues','Hypotheses Refinement under Topological Communication Constraints','Negotiation by Abduction and Relaxation','The LOGIC Negotiation Model','Bid Expressiveness and Clearing Algorithms in Multiattribute Double Auctions','(In)Stability Properties of Limit Order Dynamics','Efficiency and Nash Equilibria in a Scrip System for P2P Networks','Playing Games in Many Possible Worlds','Finding Equilibria in Large Sequential Games of Imperfect Information','Multi-Attribute Coalitional Games','The Sequential Auction Problem on eBay: An Empirical Analysis and a Solution','Networks Preserving Evolutionary Equilibria and the Power of Randomization','An Analysis of Alternative Slot Auction Designs for Sponsored Search','The Dynamics of Viral Marketing','Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering','Empirical Mechanism Design: Methods, with Application to a Supply-Chain Scenario','On the Computational Power of Iterative Auctions','Information Markets vs. Opinion Pools: An Empirical Comparison','Communication Complexity of Common Voting Rules','Complexity of (Iterated) Dominance','Hidden-Action in Multi-Hop Routing','A Price-Anticipating Resource Allocation Mechanism for Distributed Shared Clusters','From Optimal Limited To Unlimited Supply Auctions','Robust Solutions for Combinatorial Auctions','Marginal Contribution Nets: A Compact Representation Scheme for Coalitional Games','Towards Truthful Mechanisms for Binary Demand Games: A General Framework','Cost Sharing in a Job Scheduling Problem Using the Shapley Value','On Decentralized Incentive Compatible Mechanisms for Partially Informed Environments','ICE: An Iterative Combinatorial Exchange','Weak Monotonicity Suffices for Truthfulness on Convex Domains','Negotiation-Range Mechanisms: Exploring the Limits of Truthful Efficient Markets','Privacy in Electronic Commerce and the Economics of Immediate Gratification','Expressive Negotiation over Donations to Charities','Mechanism Design for Online Real-Time Scheduling','Robust Incentive Techniques for Peer-to-Peer Networks','Self-interested Automated Mechanism Design and Implications for Optimal Combinatorial Auctions','A Dynamic Pari-Mutuel Market for Hedging, Wagering, and Information Aggregation','Applying Learning Algorithms to Preference Elicitation','Competitive Algorithms for VWAP and Limit Order Trading','On Cheating in Sealed-Bid Auctions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = []\n",
    "for file_name,title in zip(filename_list,a):\n",
    "    title_dict = {}\n",
    "    title_dict['file'] = file_name\n",
    "    title_dict['title'] = title\n",
    "    title_list.append(title_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path_chunk = '/users/kanagre/AutomaticKeyphraseExtraction/SemEval2010/train/'\n",
    "import os\n",
    "abstract_list = []\n",
    "for subdir, dirs, files in os.walk(directory_path_chunk):\n",
    "    for file in sorted(files):\n",
    "        if file.endswith('.txt.final'):\n",
    "            with open(directory_path_chunk+file, 'r') as text_file:\n",
    "                file_list = text_file.readlines()\n",
    "                l = []\n",
    "                l = [file_list.index(i) for i in file_list if 'ABSTRACT' in i]\n",
    "                if not l:\n",
    "                    l = [file_list.index(i) for i in file_list if 'Abstract' in i]\n",
    "                l1 = [file_list.index(i) for i in file_list if 'Categories and Subject Descriptors' in i]\n",
    "                abstract_string = \"\"\n",
    "                b = []\n",
    "                b = [file_list.index(i) for i in file_list if 'INTRODUCTION' in i]\n",
    "                b1 = [file_list.index(i) for i in file_list if 'REFERENCES' in i]\n",
    "                body_string = \"\"\n",
    "                \n",
    "                abstract_dict = {}\n",
    "                if len(l) == 0 or len(l1) == 0:\n",
    "                    abstract_string = \"\"\n",
    "                elif len(b) == 0 or len(b1) == 0:\n",
    "                    body_string = \"\"                    \n",
    "                else:\n",
    "                    for i in range(l[0]+1, l1[0]):\n",
    "                        abstract_line = file_list[i]\n",
    "                        abstract_string = abstract_string + abstract_line\n",
    "                    for i in range(b[0]+1, b1[0]):\n",
    "                        body_line = file_list[i]\n",
    "                        body_string = body_string + body_line \n",
    "                    abstract_dict['file'] = file\n",
    "                    abstract_dict['abstract'] = abstract_string.replace('\\n', ' ')\n",
    "                    abstract_dict['body'] = body_string.replace('\\n', ' ')\n",
    "                    abstract_list.append(abstract_dict)\n",
    "                \n",
    "#                 b = []\n",
    "#                 b = [file_list.index(i) for i in file_list if '1. INTRODUCTION' in i]\n",
    "#                 b1 = [file_list.index(i) for i in file_list if '7. REFERENCES' in i]\n",
    "#                 body_string = \"\"\n",
    "#                 if len(b) == 0 or len(b1) == 0:\n",
    "#                     abstract_string = \"\"\n",
    "#                 else:\n",
    "#                     for i in range(b[0]+1, b1[0]):\n",
    "#                         body_line = file_list[i]\n",
    "#                         body_string = body_string + body_line \n",
    "#                     abstract_dict['file'] = file\n",
    "#                     abstract_dict['abstract'] = abstract_string.replace('\\n', ' ')\n",
    "#                     abstract_dict['body'] = body_string.replace('\\n', ' ')\n",
    "#                     abstract_list.append(abstract_dict)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "d = defaultdict(dict)\n",
    "for l in (abstract_list, title_list):\n",
    "    for elem in l:\n",
    "        d[elem['file']].update(elem)\n",
    "title_abstract = list(d.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

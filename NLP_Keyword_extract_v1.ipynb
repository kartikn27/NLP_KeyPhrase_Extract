{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kartik Spark Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/spark-2.1.0-bin-cdh5.9.1/\")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "    config('spark.executor.memory', '5g').\\\n",
    "    config('spark.driver.memory', '5g').\\\n",
    "    config('spark.driver.maxResultSize', '3g').\\\n",
    "    config('spark.dynamicAllocation.maxExecutors', 20).\\\n",
    "getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rahul Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/cloudera/parcels/SPARK2/lib/spark2/')\n",
    "\n",
    "# import the library\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# for Spark 2.0, we have a unified entry point to the cluster\n",
    "spark = SparkSession.builder.\\\n",
    "    getOrCreate()\n",
    "\n",
    "# for previous versions, we can simulate SparkContext and SQLContext\n",
    "sc = spark.sparkContext\n",
    "sqlContext = spark\n",
    "\n",
    "# Display information about current execution\n",
    "spark.conf.get('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addPyFile('rake.py')\n",
    "spark.sparkContext.addFile('SmartStoplist.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rake\n",
    "rake_object = rake.Rake(\"SmartStoplist.txt\", 3, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_words(text, good_tags=set(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])):\n",
    "    import itertools, nltk, string\n",
    "\n",
    "    # exclude candidates that are stop words or entirely punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokenize and POS-tag words\n",
    "    tagged_words = itertools.chain.from_iterable(nltk.pos_tag_sents(nltk.word_tokenize(sent)\n",
    "                                                                    for sent in nltk.sent_tokenize(text)))\n",
    "    # filter on certain POS tags and lowercase all words\n",
    "    candidates = [word.lower() for word, tag in tagged_words\n",
    "                  if tag in good_tags and word.lower() not in stop_words\n",
    "                  and not all(char in punct for char in word)]\n",
    "\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_keyphrases_by_textrank(text, n_keywords=0.05):\n",
    "    from itertools import takewhile, tee\n",
    "    import networkx, nltk\n",
    "    \n",
    "    # tokenize for all words, and extract *candidate* words\n",
    "    words = [word.lower()\n",
    "             for sent in nltk.sent_tokenize(text)\n",
    "             for word in nltk.word_tokenize(sent)]\n",
    "    candidates = extract_candidate_words(text)\n",
    "    # build graph, each node is a unique candidate\n",
    "    graph = networkx.Graph()\n",
    "    graph.add_nodes_from(set(candidates))\n",
    "    # iterate over word-pairs, add unweighted edges into graph\n",
    "    def pairwise(iterable):\n",
    "        \"\"\"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\"\"\n",
    "        a, b = tee(iterable)\n",
    "        next(b, None)\n",
    "        return zip(a, b)\n",
    "    for w1, w2 in pairwise(candidates):\n",
    "        if w2:\n",
    "            graph.add_edge(*sorted([w1, w2]))\n",
    "    # score nodes using default pagerank algorithm, sort by score, keep top n_keywords\n",
    "    ranks = networkx.pagerank(graph)\n",
    "    if 0 < n_keywords < 1:\n",
    "        n_keywords = int(round(len(candidates) * n_keywords))\n",
    "    word_ranks = {word_rank[0]: word_rank[1]\n",
    "                  for word_rank in sorted(ranks.items(), key=lambda x: x[1], reverse=True)[:n_keywords]}\n",
    "    keywords = set(word_ranks.keys())\n",
    "    # merge keywords into keyphrases\n",
    "    keyphrases = {}\n",
    "    j = 0\n",
    "    for i, word in enumerate(words):\n",
    "        if i < j:\n",
    "            continue\n",
    "        if word in keywords:\n",
    "            kp_words = list(takewhile(lambda x: x in keywords, words[i:i+10]))\n",
    "            avg_pagerank = sum(word_ranks[w] for w in kp_words) / float(len(kp_words))\n",
    "            keyphrases[' '.join(kp_words)] = avg_pagerank\n",
    "            # counter as hackish way to ensure merged keyphrases are non-overlapping\n",
    "            j = i + len(kp_words)\n",
    "    \n",
    "    return sorted(keyphrases.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_features(candidates, doc_text, doc_excerpt, doc_title, doc_file):\n",
    "    import collections, math, nltk, re\n",
    "    \n",
    "    candidate_scores = collections.OrderedDict()\n",
    "    \n",
    "    # get word counts for document\n",
    "    doc_word_counts = collections.Counter(word.lower()\n",
    "                                          for sent in nltk.sent_tokenize(doc_text)\n",
    "                                          for word in nltk.word_tokenize(sent))\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        \n",
    "        pattern = re.compile(r'\\b'+re.escape(candidate)+r'(\\b|[,;.!?]|\\s)', re.IGNORECASE)\n",
    "        \n",
    "        # frequency-based\n",
    "        # number of times candidate appears in document\n",
    "        cand_doc_count = len(pattern.findall(doc_text))\n",
    "        # count could be 0 for multiple reasons; shit happens in a simplified example\n",
    "        if not cand_doc_count:\n",
    "            print('**WARNING:', candidate, 'not found!')\n",
    "            continue\n",
    "    \n",
    "        # statistical\n",
    "        candidate_words = candidate.split()\n",
    "        max_word_length = max(len(w) for w in candidate_words)\n",
    "        term_length = len(candidate_words)\n",
    "        # get frequencies for term and constituent words\n",
    "        sum_doc_word_counts = float(sum(doc_word_counts[w] for w in candidate_words))\n",
    "        try:\n",
    "            # lexical cohesion doesn't make sense for 1-word terms\n",
    "            if term_length == 1:\n",
    "                lexical_cohesion = 0.0\n",
    "            else:\n",
    "                lexical_cohesion = term_length * (1 + math.log(cand_doc_count, 10)) * cand_doc_count / sum_doc_word_counts\n",
    "        except (ValueError, ZeroDivisionError) as e:\n",
    "            lexical_cohesion = 0.0\n",
    "        \n",
    "        # positional\n",
    "        # found in title, key excerpt\n",
    "        in_title = 1 if pattern.search(doc_title) else 0\n",
    "        in_excerpt = 1 if pattern.search(doc_excerpt) else 0\n",
    "        # first/last position, difference between them (spread)\n",
    "        doc_text_length = float(len(doc_text))\n",
    "        first_match = pattern.search(doc_text)\n",
    "        abs_first_occurrence = first_match.start() / doc_text_length\n",
    "        if cand_doc_count == 1:\n",
    "            spread = 0.0\n",
    "            abs_last_occurrence = abs_first_occurrence\n",
    "        else:\n",
    "            for last_match in pattern.finditer(doc_text):\n",
    "                pass\n",
    "            abs_last_occurrence = last_match.start() / doc_text_length\n",
    "            spread = abs_last_occurrence - abs_first_occurrence\n",
    "\n",
    "        candidate_scores[candidate] = {'document': doc_file,\n",
    "                                       'term_count': cand_doc_count,\n",
    "                                       'term_length': term_length, \n",
    "                                       'max_word_length': max_word_length,\n",
    "                                       'spread': spread, \n",
    "                                       'lexical_cohesion': lexical_cohesion,\n",
    "                                       'in_excerpt': in_excerpt, \n",
    "                                       'in_title': in_title,\n",
    "                                       'abs_first_occurrence': abs_first_occurrence,\n",
    "                                       'abs_last_occurrence': abs_last_occurrence}\n",
    "\n",
    "    return candidate_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
